# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/trinanjan12/Image-Based-Virtual-Try-on-Network-from-Unpaired-Data/blob/master/inference/test.ipynb
"""

import os
from collections import OrderedDict
from torch.autograd import Variable
from options.test_options import TestOptions
from data.ov_test_dataset import TestDataset
from models.models import create_model
import util.util as util
from util.visualizer import Visualizer
from util import html
import torch
from torchvision import transforms
from torch.utils.data import DataLoader

opt = TestOptions().parse(save=False)
opt.nThreads = 1   # test code only supports nThreads = 1
opt.batchSize = 1  # test code only supports batchSize = 1
opt.serial_batches = True  # no shuffle
opt.no_flip = True  # no flip

opt.label_feat = True
opt.model = 'ov_pix2pixHD'
opt.name = 'zalando_shape'
opt.dataroot= './datasets/test_data/'
opt.shape_generation = True
# opt.use_generator_last_activation = False
opt.input_nc= 20 
opt.output_nc = 20

# data_loader = CreateDataLoader(opt)
# dataset = data_loader.load_data()
augment = {}
augment['1'] = transforms.Compose(
    [
        transforms.ToPILImage(),
        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=20),
        transforms.ToTensor(),
        transforms.Normalize((0.5, ), (0.5, ))])  # change to [C, H, W]

augment['2'] = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.5, ), (0.5, ))])  # change to [C, H, W]

test_dataset = TestDataset(opt, augment)

test_dataloader = DataLoader(test_dataset,
                              shuffle=False,
                              drop_last=False,
                              num_workers=6,
                              batch_size=opt.batchSize,
                              pin_memory=True)
dataset_size = len(test_dataset)
print('#testing images = %d' % dataset_size)

## FOR DEBUGGING 
# input_dict = {
#             'query_parse_map': A_tensor_label,
#             'ref_parse_map': B_tensor_label,
#             'query_seg_map': query_label_seg_mask,
#             'ref_seg_map': ref_label_seg_mask,
#             'query_img': A_tensor_img,
#             'ref_img': B_tensor_img,
#             'C_tensor': C_tensor
#         }
# for i in test_dataset[0].keys():
#     try:
#         print('{}-------'.format(i),test_dataset[0][i].shape)
#     except Exception as e:
#         print(e)

model = create_model(opt)

# torch.unique(test_dataset[0]['C_tensor'])

# from matplotlib import pyplot as plt
# from PIL import Image
# from PIL import Image
# import numpy as np
# x = test_dataset[0]['C_tensor'].cpu().float().numpy()
# for i in range(20):
#     plt.show()
#     plt.imshow(Image.fromarray((x[i,:,:]).astype(np.uint8)))

visualizer = Visualizer(opt)
# create website
web_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))
webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))
for i, data in enumerate(test_dataloader):
    if i >= opt.how_many:
        break
    query_ref_mixed, generated = model.inference_forward_shape(data['query_parse_map'],data['ref_parse_map'],
                                                              data['C_tensor'])
    visuals = OrderedDict([('query', util.tensor2label(data['query_parse_map'][0], opt.label_nc)),
                           ('ref', util.tensor2label(data['ref_parse_map'][0], opt.label_nc)),
                           ('query_ref_mixed', util.tensor2label(query_ref_mixed.data[0], opt.label_nc)),
                           ('synthesized_Simage', util.tensor2label(generated.data[0], opt.label_nc)),
                          ('synthesized_image_edgemap', util.tensor2edgemap(torch.softmax(generated.data[0],dim=0)))])
    img_path = data['path']
    print('process image... %s' % img_path)
    visualizer.save_images(webpage, visuals, img_path)

webpage.save()

# import numpy as np
# np.unique(visuals['synthesized_image_edgemap'])

# from PIL import Image
# test = Image.open('./datasets/fashion_compatibility/test_query_ref_label/0_N00098_synthesized_image_edgemap.jpg')
# print(np.unique(np.array(test)))
# test = Image.open('./results/fashion_compatibility_shape/test_latest/images/0_N00098_synthesized_image_edgemap.png')
# np.unique(np.array(test))

import os
from collections import OrderedDict
from torch.autograd import Variable
from options.test_options import TestOptions
from data.ov_test_dataset import TestDataset
from models.models import create_model
import util.util as util
from util.visualizer import Visualizer
from util import html
import torch
from torchvision import transforms
from torch.utils.data import DataLoader

opt = TestOptions().parse(save=False)
opt.nThreads = 1   # test code only supports nThreads = 1
opt.batchSize = 1  # test code only supports batchSize = 1
opt.serial_batches = True  # no shuffle
opt.no_flip = True  # no flip

opt.label_feat = True
opt.model = 'ov_pix2pixHD'
opt.name = 'zalando_appearance'
# opt.dataroot= './datasets/fashion_compatibility/'
opt.appearance_generation = True
opt.use_generator_last_activation = True
opt.input_nc= 20 
opt.output_nc = 3

# data_loader = CreateDataLoader(opt)
# dataset = data_loader.load_data()
augment = {}
augment['1'] = transforms.Compose(
    [
        transforms.ToPILImage(),
        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=20),
        transforms.ToTensor(),
        transforms.Normalize((0.5, ), (0.5, ))])  # change to [C, H, W]

augment['2'] = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.5, ), (0.5, ))])  # change to [C, H, W]

test_dataset = TestDataset(opt, augment)

test_dataloader = DataLoader(test_dataset,
                              shuffle=False,
                              drop_last=False,
                              num_workers=6,
                              batch_size=opt.batchSize,
                              pin_memory=True)
dataset_size = len(test_dataset)
print('#testing images = %d' % dataset_size)

# # FOR DEBUGGING 
# input_dict = {
#             'query_parse_map': A_tensor_label,
#             'ref_parse_map': B_tensor_label,
#             'query_seg_map': query_label_seg_mask,
#             'ref_seg_map': ref_label_seg_mask,
#             'query_img': A_tensor_img,
#             'ref_img': B_tensor_img,
#             'C_tensor': C_tensor
#         }
# for i in test_dataset[0].keys():
#     try:
#         print('{}-------'.format(i),test_dataset[0][i].shape)
#     except Exception as e:
#         print(e)

model = create_model(opt)

visualizer = Visualizer(opt)
# create website
web_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))
webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))
for i, data in enumerate(test_dataloader):
    if i >= opt.how_many:
        break
    generated = model.inference_forward_appearance(data['query_img'],data['query_parse_map'],
                                                              data['query_seg_map'],data['ref_img'],
                                                              data['ref_parse_map'],data['ref_seg_map'],
                                                              data['C_tensor'])
    
    visuals = OrderedDict([('query_img', util.tensor2im(data['query_img'][0])),
                           ('ref_image', util.tensor2im(data['ref_img'][0])),
                           ('generated_parse_map', util.tensor2label(data['C_tensor'][0], opt.label_nc)),
                           ('synthesized_image', util.tensor2im(generated.data[0]))])
    
    img_path = data['path']
    print('process image... %s' % img_path)
    visualizer.save_images(webpage, visuals, img_path)

webpage.save()

app_feature_vectors_query = torch.zeros((20,30)).float().cuda()

app_feature_vectors_query[0,0] = 1

import os
from collections import OrderedDict
from torch.autograd import Variable
from options.test_options import TestOptions
from data.ov_test_dataset import TestDataset
from models.models import create_model
import util.util as util
from util.visualizer import Visualizer
from util import html
import torch
from torchvision import transforms
from torch.utils.data import DataLoader

opt = TestOptions().parse(save=False)
opt.nThreads = 1   # test code only supports nThreads = 1
opt.batchSize = 1  # test code only supports batchSize = 1
opt.serial_batches = True  # no shuffle
opt.no_flip = True  # no flip

opt.model = 'ov_pix2pixHD'
opt.name = 'fashion_compatibility_appearance'
# opt.dataroot= './datasets/fashion_compatibility/'
opt.appearance_generation = True
opt.use_generator_last_activation = True
opt.input_nc= 20 
opt.output_nc = 3

# data_loader = CreateDataLoader(opt)
# dataset = data_loader.load_data()
augment = {}
augment['1'] = transforms.Compose(
    [
        transforms.ToPILImage(),
        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=20),
        transforms.ToTensor(),
        transforms.Normalize((0.5, ), (0.5, ))])  # change to [C, H, W]

augment['2'] = transforms.Compose(
    [
        transforms.ToTensor(),
        transforms.Normalize((0.5, ), (0.5, ))])  # change to [C, H, W]

test_dataset = TestDataset(opt, augment)

test_dataloader = DataLoader(test_dataset,
                              shuffle=False,
                              drop_last=False,
                              num_workers=6,
                              batch_size=opt.batchSize,
                              pin_memory=True)
dataset_size = len(test_dataset)
print('#testing images = %d' % dataset_size)

# # FOR DEBUGGING 
# input_dict = {
#             'query_parse_map': A_tensor_label,
#             'ref_parse_map': B_tensor_label,
#             'query_seg_map': query_label_seg_mask,
#             'ref_seg_map': ref_label_seg_mask,
#             'query_img': A_tensor_img,
#             'ref_img': B_tensor_img,
#             'C_tensor': C_tensor
#         }
# for i in test_dataset[0].keys():
#     try:
#         print('{}-------'.format(i),test_dataset[0][i].shape)
#     except Exception as e:
#         print(e)

import os
from collections import OrderedDict
from torch.autograd import Variable
from options.train_options import TrainOptions
from data.ov_test_dataset import TestDataset
from models.models import create_model
import util.util as util
from util.visualizer import Visualizer
from util import html
import torch
from torchvision import transforms
from torch.utils.data import DataLoader
import time

opt = TrainOptions().parse(save=False)
opt.nThreads = 1   # test code only supports nThreads = 1
opt.batchSize = 1  # test code only supports batchSize = 1
opt.serial_batches = True  # no shuffle
opt.no_flip = True  # no flip

opt.model = 'ov_pix2pixHD_online'
opt.name = 'fashion_compatibility_online'
# opt.dataroot= './datasets/fashion_compatibility/'
opt.appearance_generation = True
opt.use_generator_last_activation = True
opt.input_nc= 20 
opt.output_nc = 3
opt.load_pretrain = './checkpoints/zalando_appearance/'

model = create_model(opt)

# Training Visualizer
visualizer = Visualizer(opt)

# Optimizers
optimizer_G, optimizer_D = model.module.optimizer_G, model.module.optimizer_D

prev_j = 0
index_check = 20
for i, data in enumerate(test_dataloader, start=0):
    for j in range(0,100):
        save_fake = True if j % index_check == 0 else False
        losses, generated = model(data['query_img'],data['query_parse_map'],
                                                    data['query_seg_map'],data['ref_img'],
                                                    data['ref_parse_map'],data['ref_seg_map'],
                                                    data['C_tensor'],infer=save_fake)

        # sum per device losses
        losses = [torch.mean(x) if not isinstance(x, int) else x for x in losses]
        loss_dict = dict(zip(model.module.loss_names, losses))

        # calculate final loss scalar
        loss_ref = (loss_dict['G_GAN_REF'] + loss_dict['G_VGG'])
        loss_query = loss_dict['G_GAN_QUERY']
        loss_online = loss_ref + loss_query

        ############### Backward Pass ####################
        # update generator weights
        optimizer_G.zero_grad()
        loss_online.backward()
        optimizer_G.step()
        
        print('loss_ref {}, loss_query {}, loss_online {}'.format(loss_ref,loss_query,loss_online))
        
        ############## Display results and errors ##########
        if save_fake:
            visuals = OrderedDict([('query_img', util.tensor2im(data['query_img'][0])),
                               ('ref_image', util.tensor2im(data['ref_img'][0])),
                               ('synthesized_image', util.tensor2im(generated.data[0]))])
            visualizer.display_current_results(visuals, (j // index_check) + prev_j, i)
    prev_j = (j // index_check)*(i+1)

# Test exercises

import numpy as np
from PIL import Image

image_numpy = test_dataset[2]['ref_img'].cpu().float().numpy()
image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0
image_numpy = image_numpy.astype(np.uint8)
Image.fromarray(image_numpy)

org_seg_map = test_dataset[2]['ref_parse_map'].clone()

# unnormalize it 
unnormalize_seg_map = org_seg_map.cpu().float().numpy()
unnormalize_seg_map = (np.transpose(unnormalize_seg_map, (1, 2, 0)) + 1) / 2.0
unnormalize_seg_map = unnormalize_seg_map.astype(np.uint8)
np.unique(unnormalize_seg_map)

# filter the top section
filter_part = unnormalize_seg_map[:, :, 5] + unnormalize_seg_map[:, :, 6] + unnormalize_seg_map[ :, :,7]  # 5,6,7,
filter_part = filter_part > 0
filter_part = np.expand_dims(filter_part,-1)


image_numpy = test_dataset[2]['ref_img'].cpu().float().numpy()
image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0
image_numpy = image_numpy.astype(np.uint8)
source_img = image_numpy * filter_part # other part normalized 0 --> 127 gray
Image.fromarray(source_img)

ref_parse_map_online = torch.zeros((20, 512, 256)).float().cuda()
for num_seg_channel in range(20):
    if 4 < num_seg_channel < 8:
        ref_parse_map_online[num_seg_channel,:,:] = test_dataset[2]['ref_parse_map'][num_seg_channel,:,:]

ref_parse_map_online = (ref_parse_map_online + 1)/2
filter_part = ref_parse_map_online[5, :, :] + ref_parse_map_online[6, :, :] + ref_parse_map_online[7, :, :]  # 5,6,7,
filter_part = filter_part > 0
filter_part = torch.unsqueeze(filter_part,0).float().cuda()
src = test_dataset[2]['ref_img'].float().cuda()
src = (src + 1)/2
source_img = src * filter_part # other part normalized 0 --> 127 gray
test = source_img.clone()
image_numpy = source_img.cpu().float().numpy()
image_numpy = np.transpose(image_numpy,(1, 2, 0))* 255.0
image_numpy = image_numpy.astype(np.uint8)
Image.fromarray(image_numpy)

norm = transforms.Normalize((0.5,), (0.5,))
out = norm(test)
torch.unique(out)

